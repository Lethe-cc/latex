\documentclass{IEEEtran}
\usepackage[T1]{fontenc} % optional
\usepackage{amsmath}
\usepackage[cmintegrals]{newtxmath}
\usepackage{bm} % optional
\usepackage{graphicx}
\usepackage{float}

\newtheorem{prop}{Proposition}
\newtheorem{theo}{Theorem}
\newtheorem{nota}{Notation}
\newtheorem{defi}{Definition}
\newtheorem{conv}{Convention}
\newtheorem{maxthe}{Max-Flow Min-Cut Theorem}
\newtheorem{exam}{Example}
\newtheorem{proof}{\hskip 2em Proof}
\newtheorem{lemm}{Lemma}
\newtheorem{rema}{Remark}

\begin{document}
\title{Linear Network Coding}
	\author{Shuo-Yen Robert Li,\ \IEEEmembership{Senior Member,IEEE,
	} Raymond W. Yeung,\ \IEEEmembership{Fellow,IEEE,} and Ning Cai\\
	Zhiwei Chen 1016041134
	\thanks{Transcribed by Zhiwei Chen, April 8, 2017.}}
\maketitle
	

\begin{abstract}
	Consider a communication network in which certain source nodes multicast information to other nodes on the network in the multihop fashion where every node can pass on any of its received data to others. We are interested in how fast each node can receive the complete information, or equivalently, what the information rate arriving at each node is. Allowing a node to encode its received data before passing it on, the question involves optimization of the multicast mechanisms at the nodes. Among the simplest coding schemes is linear coding, which regards a block of data as a vector over a certain base field and allows a node to apply a linear transformation to a vector before passing it on. We formulate this multicast problem and prove that linear coding suffices to achieve the optimum, which is the max-flow from the source to each receiving 	node.
\end{abstract}
\begin{IEEEkeywords}
	Coding, network routing, switching.
\end{IEEEkeywords}

\section{Introduction}\label{sec:introduction}
\IEEEPARstart{D}{efine} a \emph{communication network} as a pair $(G,S)$, where $S$ is a finite directed multigraph and $S$ is the unique node in $G$ without any incoming edges. A directed edge in $G$ is called a channel in the communication network $(G,S)$, The special node $S$ is called the source, while every other node may serve as a sink as we shall explain.
\par
A channel in graph $G$ represents a noiseless communication link on which one unit of information (e.g., a bit) can be transmitted per unit time. The multiplicity of the channels from a node $X$ to another node Y represents the capacity of direct transmission from $X$ to $Y$. In other words, every single channel has unit capacity.
\par
At the source $S$, a finite amount of information is generated and multicast to other nodes on the network in the multihop fashion where every node can pass on any of its received data to other nodes. At each nonsource node which serves as a sink, the complete information generated at $S$ is recovered. We are naturally interested in how fast each sink node can receive the complete information.
\par
As an example, consider the multicast of two data bits, $b_1$ and $b_2$, from the source $S$ in the communication network depicted by Fig. 1(a) to both nodes $Y$ and $Z$. One solution is to let the channels $ST$, $TY$, $TW$, and $WZ$ carry the bit $b_1$ and channels $SU$, $UZ$, $UW$, and $WY$ carry the bit $b_2$. Note that in this scheme, an intermediate node sends out a data bit only if it receives the same bit from another node. For example, the node $T$ receives the bit $b_1$ and sends a copy on each of the two channels $TY$ and $TW$. Similarly, the node $U$ receives the bit $b_2$ and sends a copy into each of the two channels $UW$ and $UZ$. In our model, we assume that there is no processing delay at the intermediate nodes.
\begin{figure}[H]
	\centering
	\includegraphics[width=2.5in]{fig1}
	\caption{Two communication networks.}
	\label{fig1}
\end{figure}
\par
Unlike a conserved physical commodity, information can be replicated or coded. Introduced in [1] (see also [5, Ch. 11]), the notion of network coding refers to coding at the intermediate nodes when information is multicast in a network. Let us now illustrate network coding by considering the communication network depicted by Fig. 1(b). In this network, wewant to multicast two bits $b_1$ and $b_2$ from the source $S$ to both the nodes $Y$ and $Z$. A solution is to let the channels $ST$, $TW$, $TY$ carry the bit $b_1$, channeels $SU$, $UW$, $UZ$ carry the bit $b_2$, and channels $WX$, $XY$, $XZ$ carry the exclusive-or $b_1 \oplus b_2$. Then, the node $Y$ receives $b_1$ and $b_1 \oplus b_2$, from which the bit $b_2$ can be decoded. Similarly, the node $Z$ can decode the bit $b_1$ from and $b_2$.The coding/decoding scheme is assumed to have been agreed upon beforehand.
\par
It is not difficult to see that the above scheme is the only solution to the problem. In other words, without network coding, it is impossible to multicast two bits per unit time from the source $S$ to both the nodes $Y$ and $Z$. This shows the advantage of network coding. In fact, replication of data can be regarded as a special case of network coding.
\par
As we have pointed out, the natures of physical commodities and information are very different. Nevertheless, both of them are governed by certain laws of flow. For the network flow of a physical commodity, we have the following.
\begin{itemize}
	\item The law of commodity flow: The total volume of the outflow from a nonsource node cannot exceed the total volume of the inflow.
\end{itemize}
The counterpart for a communication network is as follows.
\begin{itemize}
	\item The law of information flow: The content of any information flowing out of a set of nonsource nodes can be derived from the accumulated information that has flown into the set of nodes.
\end{itemize}
After all, information replication and coding do not increase the information content.
\par
The information rate from the source to a sink can potentially become higher and higher when the permitted class of coding schemes is wider and wider. However, the law of information flow limits this information rate to the max-flow (i.e., the maximum commodity flow) from the source to that particular sink for a very wide class of coding schemes. The details are given in [1].
\par
It has been proved in [1] that the information rate from the source to a set of nodes can reach the minimum of the individual max-flow bounds through coding. In the present paper, we shall prove constructively that by linear coding alone, the rate at which a message reaches each node can achieve the individual max-flow bound. (This result is somewhat stronger than the one in [1]. Please refer to the example in Section III.) More explicitly, we treat a block of data as a vector over a certain base field and allow a node to apply a linear transformation to a vector before passing it on. A preliminary version of this paper has appeared in the conference proceedings [3].
\par
The remainder of the paper is organized as follows. In Section II, we introduce the basic notions, in particular, the notion of a \emph{linear-code multicast} (LCM). In Section III, we show that with a "generic" LCM, every node can simultaneously receive information from the source at rate equal to its max-flow bound. In Section IV, we describe the physical implementation of an LCM first when the network is acyclic and then when the network is cyclic. In Section V, we present a greedy algorithm for constructing a generic LCM for an acyclic network. The same algorithm can be applied to a cyclic network by expanding the network into an acyclic network. This results in a "timevarying" LCM, which, however, requires high complexity in implementation. In Section VI, we introduce the time-invariant LCM(TILCM) and present a heuristic for constructing a generic TILCM. Section VII presents concluding remarks.

\section{BASIC NOTIONS}\label{sec:basic}
\par
In this section, we first introduce some graph-theoretic terminology and notations which will be used throughout the paper. Then we will introduce the notion of an LCM, an abstract algebraic description of a linear code on a communication network.
\par
\emph{Convention}: The generic notation for a nonsource node will be $T$, $U$, $W$, $X$, $Y$, or $Z$. The notation $XY$ will stand for any channel from $X$ to $Y$.
\par
\emph{Definition}Over a communication network a \emph{flow} from the source to a nonsource node $T$ is a collection of channels, to be called the busy channels in the flow, such that
\begin{itemize}
	\item[1)] the subnetwork defined by the busy channels is acyclic, i.e., the busy channels do not form directed cycles;
	\item[2)] for any node other than $S$ and $T$, the number of incoming busy channels equals the number of outgoing 	busy channels;
	\item[3)] the number of outgoing busy channels from $S$ equals the number of incoming busy channels to $T$.
\end{itemize}
\par
In other words, a flow is an \emph{acyclic} collection of channels that abides by the law of commodity flow. The number of outgoing busy channels $S$ from will be called the volume of the flow. The node $T$ is called the \emph{sink} of the flow. All the channels on the communication network that are not busy channels of the flow are called the \emph{idle} channels with respect to the flow.

\begin{prop}
The busy channels in a flow with volume $f$ can be partitioned into $f$ simple paths from the source to the sink.
\end{prop}
\par The proof of this proposition is omitted.
\begin{nota}
	For every nonsource node $T$ on a network $(G,S)$, the maximum volume of a flow from the source to $T$ is denoted as $\text{maxflow}_G(T)$, or simply $\text{maxflow}(T)$when there is no ambiguity.
\end{nota}
\begin{defi}
	A cut on a communication network $(G,S)$, between the source and a nonsource node $(T)$ means a collection $C$ of nodes which includes $S$ but not $T$. A channel $XY$ is said to be \emph{in} the cut $C$ if $X \in C$ and $y \notin C$. The number of channels in a cut is called the \emph{value} of the cut.
\end{defi}
\par
The well-known Max-Flow Min-Cut Theorem (see, for example, [2, Ch. 4, Theorem 2.3]) still applies despite the acyclic restriction in the definition of a flow.
\begin{maxthe}
	For every nonsource node $T$, the minimum value of a cut between the source and a node $T$ is equal to $\text{maxflow}(T)$.
\end{maxthe}

We are now ready to define a linear code multicast.
\begin{nota}
	Let $d$ be the maximum of $\text{maxflow}(T)$ over all $T$. Throughout Sections IIâ€“V, the symbol $\Omega$ will denote a fixed-dimensional vector space over a sufficiently large base field.
\end{nota}
\begin{conv}
	The information unit is taken as a symbol in the base field. In other words, $1$ symbol in the base field can be transmitted on a channel every unit time.
\end{conv}

\begin{defi}
	A \emph{linear-code multicast (LCM)} $v$ on a communication network $(G,S)$ is an assignment of a vector space $v(X)$	to every node $X$ and a vector $v(XY)$ to every channel $XY$ such that
	\begin{itemize}
		\item[1)] $v(S)=\Omega$;
		\item[2)] $v(XY) \in v(X) \text{for every channel } XY$;
		\item[3)] for any collection $\wp$ of nonsource nodes in the network\\
		$ \left \langle \left \{ v(T):T\in \wp \right \} \right \rangle=\left \langle \left \{ v(XY):X\notin \wp,Y\in \wp \right \} \right \rangle$.
	\end{itemize}
\end{defi}
\par 
The notation $\left \langle \bullet \right \rangle $ is for linear span. Condition 3) says that the vector spaces $v(T)$ on all nodes $T$ inside $\wp$ together have the same linear span as the vectors $v(XY)$ on all channels $XY$ to nodes in $\wp$ from outside of $\wp$. Conditions 2) and 3) show that an LCM abides by the law of information flow stated in Section I. The vector $v(XY)$ assigned to every channel $XY$ may be identified with a $d$-dimensional \emph{column} vector over the base field of $\Omega$ by choosing a basis for $\Omega$.
\par 
Applying Condition 3) to the collection of a single nonsource node $T$, the space $v(T)$ is linearly spanned by vectors $v(XT)$ on all incoming channels $XY$ to $T$. This shows that an LCM on a communication network is completely determined by the vectors it assigns to the channels. Together with Condition 2), we have
\begin{itemize}
	\item[4)] The vector assigned to an outgoing channel from $T$ must be a linear combination of the vectors assigned to the incoming	channels of $T$.
\end{itemize}
Condition 4) may be regarded as the "\emph{law of information flow at a node.}" However, unlike in network flow theory, the law of information flow being observed for every single node does not necessarily imply it being observed for every set of nodes when the network contains a directed cycle. A counterexample will appear in Section IV.
\par 
An LCM $v$ specifies a mechanism for data transmission over the network as follows. We encode the information to be transmitted from $S$ as a $d$-dimensional \emph{row} vector, which we shall call the \emph{information vector}. Under the transmission mechanism prescribed by the LCM $v$, the data flowing on a channel $XY$ is the matrix product of the information (row) vector with the (column) vector $v(XY)$. In this way, the vector $v(XY)$ acts as the kernel in the linear encoder for the channel $XY$. As a direct consequence of the definition of an LCM, the vector assigned to an outgoing channel from a node $X$ is a linear combination of the vectors assigned to the incoming channels to $X$. Consequently, the data sent on an outgoing channel from a node $X$ is a linear combination of the data sent on the incoming channels to $X$.
\par 
Under this mechanism, the amount of information reaching a node $T$ is given by the dimension of the vector space $v(T)$ when the LCM $v$ is used. The physical realization of this mechanism will be discussed in Section IV.
\begin{exam}
	Consider the multicast of two bits, $b_1$ and $b_2$, from $S$ to $Y$ and $Z$ in the communication network in Fig. 1(b).This is achieved with the LCM $v$ specified by
	$$ v(ST)=v(TW)=v(TY)=\begin{pmatrix} 1 \\ 0 \end{pmatrix}$$
	$$ v(SU)=v(UW)=v(UZ)=\begin{pmatrix} 0 \\ 1 \end{pmatrix}$$
	and
	$$ v(WX)=v(XY)=v(XZ)=\begin{pmatrix} 1 \\ 1 \end{pmatrix}$$
	The data sent on a channel is the matrix product of the \emph{row} vector $(b_1 \ b_2) $with the \emph{column} vector assigned to that channel by $v$. For instance, the data sent on the channel $WX$ is $b_1+b_2$. Note that, in the special case when the base field of $\Omega$ is $GF(2)$, the vector $b_1+b_2$ reduces to the exclusive-OR in an earlier $b_1\oplus b_2$ example.
\end{exam}
\begin{prop}
	For every LCM $v$ on a network, for all nodes $T$
	$$ \text{dim}(v(T)) \leq \text{maxflow}(T).$$
\end{prop}
\begin{proof}
	Fix a nonsource node $T$ and \emph{any}cut $C$ between the source and $T$
	$$v(T)\in \left \langle v(Z):Z\notin C \right \rangle = \left \langle v(YZ):Y\in C \text{and} Z\notin C \right \rangle.$$
	Hence, $\text{dim}(v(T))\leq \text{dim}(\left \langle v(YZ):Y\in C \text{and} Z\notin C \right \rangle$, which is at most equal to the value of the cut. In particular,$\text{dim}(v(T))$ is upper-bounded by the minimum value of a cut between $S$ and $T$, which by the Max-Flow Min-Cut Theorem is equal to $\text{maxflow}(T)$.
\end{proof}
\par 
This corollary says that $\text{maxflow}(T)$ is an upper bound on the amount of information received by $T$ when an LCM is used.

\section{ACHIEVING THE MAX-FLOW BOUND THROUGH A GENERIC LCM}\label{sec: achieving}
\par
In this section, we derive a sufficient condition for an LCM $v$ to achieve the max-flow bound on $\text{dim}(v(T))$ in Proposition 2.3.
\begin{defi}
	An LCM $v$ on a communication network is said to be \emph{generic} if the following condition holds for any collection of channels $X_1Y_1, X_2Y_2, \dots X_mY_m$ for $1\leq m \leq d: (*)v(X_k)\nsubset \left \langle \left \{ v(X_jY_j):j \neq k\right \} \right \rangle$ for $1\leq k \leq m$ if and only if the vector $v(X_1Y_1), v(X_2Y_2), \dots v(X_mY_m)$ are linearly independent.
\end{defi}
\par
if $v(X_1Y_1), v(X_2Y_2), \dots v(X_mY_m)$ are linearly independent, then $v(X_k)\nsubset \left \langle \left \{ v(X_jY_j):j \neq k\right \} \right \rangle$ since $v(X_kY_k)\in v(X_k)$. A generic LCM requires that the converse is also true. In this sense, a generic LCM assigns vectors which are as linearly independent as possible to the channels.

\begin{exam}
	With respect to the communication network in Fig. 1(b), the LCM $v$ in Example 2.2 is a generic LCM. However, the LCM $u$ defined by
	$$ u(ST)=u(TW)=u(TY)=\begin{pmatrix} 1 \\ 0 \end{pmatrix}$$
	$$ u(SU)=u(UW)=u(UZ)=\begin{pmatrix} 0 \\ 1 \end{pmatrix}$$
	and
	$$ u(WX)=u(XY)=u(XZ)=\begin{pmatrix} 1 \\ 0 \end{pmatrix}$$
	is not generic. This is seen by considering the set of channels ${ST,WX}$ where
	$$u(S)=u(W)=\left \langle \begin{pmatrix} 1 \\ 0 \end{pmatrix}, \begin{pmatrix} 0 \\ 1 \end{pmatrix} \right \rangle$$
	Then $u(S)\nsubset \left \rangle u(WX) \right \rangle$ and $u(W)\nsubset \left \rangle u(ST) \right \rangle$, but $u(ST)$ and $u(WX)$ are not linearly independent. Therefore, is not generic.
\end{exam}
\begin{lemm}
	Let $v$ be a generic LCM. Any collection of channels $X_1Y_1, X_2Y_2, \dots X_mY_m$ from a node $X$ with $m \leq \text{dim}(v(X))$ must be assigned linearly independent vectors by $v$.
\end{lemm}
\begin{theo}
	If $v$ is a generic LCM on a communication network, then for all nodes $T$
	$$\text{dim}(v(T))=\text{maxflow}(T).$$
\end{theo}
\begin{proof}
	Consider a node $T$ not equal to $S$. Let $f$ be the common value of $\text{maxflow){T}}$ and the minimum value of a cut between $S$ and $T$. The inequality $\text{dim}(v(T))\leq f$ follows from Proposition 2.3. Thus, we only have to show that $\text{dim}(v(T))\geq f$.
	\par let $\text{dim}(C)=\text{dim}(\left \langle v(X,Y):X\in C \text{and} Y\notin C\right \rangle)$ for any cut $C$ between $S$ and $T$. We stil show that $\text{dim}(v(T)) \geq f$ by contradiction. Assume $\text{dim}(v(T)) < f$ and let $A$ be the collection of cuts $U$ between $S$ and $T$ such that $\text{dim}(U)<f$. Since $\text{dim}(v(T)) < f$ and let $A$ implies $V\setminus \{T\}\in A$, where $V$ is the set of all the nodes in $G$, $A$ is nonempty.
	\par By the assumption that $v$ is a generic LCM, the number of edges out of $S$ is at least $d$, and $\text{dim}(\{S\})=d\geq f$. Therefore, $\{S\}\notin A$. Then there must exist a minimal member $U\in A$ in the sense that for any $Z\in U\setminus \{S\} \neq \emptyset$, $U\setminus \{Z\} \notin A$. Clearly, $U\neq \{S\}$ because $\{S\}\notin A$.
	\par Let $K$ be the set of channels in cut $U$ and $B$ be the set of boundary nodes of $U$, i.e., $Z\in B$ if and only if $Z\in U$ and there 	is a channel $(Z,Y)$, such that $Y\notin U$. Then for all $W\in B$,
	$$v(W)\nsubset \left \langle v(X,Y):(X,Y)\in K \right \rangle$$
	which can be seen as follows. The set of channels in cut $U\setminus {W}$ but not in is given by $\left \{ (X,W):X\in U\setminus{W}\right \}$. Since $v$ is an LCM
	$$\left \langle v(X,W): X\in U\setminus {W}\right \rangle \subset v(W).$$
	If $v(W)\subset \left \langle v(X,Y):(X,Y)\in K\right \rangle$, then
	$$\left \langle v(X',Y'):X'\in U\setminus \{W\},Y'\notin U\setminus \{W\}\right \rangle$$
	the subspace spanned by the channels in cut $U\setminus \{W\}$, is contained by $\left \langle v(X,Y):(X,Y)\in K\right \rangle$. This implies
	$$\text{dim}(U\setminus \{W\})\leq\text{dim}(U)<f$$
	a contradiction.
	\par Therefore, for all $W\in B$, $v(W)\nsubset \left \langle v(X,Y):(X,Y)\in K\right \rangle$. For all $(W,Y)\in K$, since
	\begin{align*}
	&\left \langle v(X,Z):(X,Z)\in K\setminus {(W,Y)}\right \rangle \subset \left \langle v(X,Y):(X,Y)\in K\right \rangle, \\
	&v(W)\nsubset \left \langle v(X,Y):(X,Y)\in K\right \rangle
	\end{align*}
	implies
	$$v(W)\nsubset \left \langle v(X,Z):(X,Z)\in K \setminus\{(W,Y)\}\right \rangle.$$
	Then, by the definition of a generic LCM, ${v(X,Y):(X,Y)\in K}$ is a collection of vectors such that $\text{dim}(U)=\text{min}(|K|,d)$. Finally, by the Max-Flow Min-Cut Theorem, $|K|\geq f$, and since $d\geq f,\text{dim}(U)\geq f$. This is a contradiction to the assumption that $U\in A$. The theorem is proved.
\end{proof}
\par An LCM for which $\text{dim}(v(T))=\text{maxflow}(T)$ for all $T$ provides a way for broadcasting a message generated at the source $S$ for which every nonsource node $T$ receives the message at rate equal to $\text{maxflow}(T)$. This is illustrated by the next example,which is based upon the assumption that the base field of $\Omega$ is an infinite field or a sufficiently large finite field. In this example, we employ a technique which is justified by the following lemma.
\begin{lemm}
	Let $X$, $Y$ and $Z$ be nodes such that $\text{maxflow}(X)=i,\text{maxflow}(Y)=j$, and $\text{maxflow}(Z)=k$, where $i\leq j$ and $i>k$. By removing any edge $UX$ in the graph, $\text{maxflow}(X)$ and $\text{maxflow}(Y)$ are reduced by at most $1$, and $\text{maxflow}(Z)$ remains unchanged.
\end{lemm}
\begin{proof}
	By removing an edge $UX$, the value of a cut $C$ between the source $S$ and node $X$(respectively, node $Y$) is reduced by $1$ if edge $UX$ is in $C$, otherwise, the value of $C$ is unchanged. By the Max-Flow Min-Cut Theorem, we see that $\text{maxflow}(X)$ and $\text{maxflow}(Y)$ are reduced by at most $1$ when edge $UX$ is removed from the graph. Now consider the value of a cut $C$ between the source $S$ and node $Z$. If $C$ contains node $X$, then edge $UX$ is not in $C$, and, therefore, the value of $C$ remains 	unchanged upon the removal of edge $UX$. If $C$ does not contain node $X$, then $C$ is a cut between the source $S$ and node $X$. By the Max-Flow Min-Cut Theorem, the value of $C$ is at least $i$. Then upon the removal of edge $UX$, the value of $C$ is lower-bounded by $i-1\geq k$. Hence, by the Max-Flow Min-Cut Theorem, $\text{maxflow}(Z)$ remains to be $k$ upon the removal of edge $UX$. The lemma is proved.
\end{proof}
\begin{exam}
	Consider a communication network for which $\text{maxflow}(T)=4,3,\text{or}1$ for nodes $T$ in the network. The source $S$ is to broadcast 12 symbols $a_1, \dots, a_{12}$ taken from a sufficiently large base field $F$. (Note that 12 is the least common 	multiple of 4, 3, and 1.) Define the set
	$$\Im_i=\{T:\text{maxflow}(T)=i\},\qquad \text{for }i=4,3,1.$$
	For simplicity, we use the second as the time unit. We now describe how $a_1, \dots, a_{12}$ can be broadcast to the nodes in $\Im_4$, $\Im_3$, $\Im_1$	in 3, 4, and 12 s, respectively, assuming the existence of an LCM on the network for $d=4,3,1$.
	\begin{itemize}
		\item[a)] Let $v_1$ be an LCM on the network with $d=1$. Let 
		$$\alpha_1=(a_1\ a_2\ a_3\ a_4)$$
		$$\alpha_2=(a_5\ a_6\ a_7\ a_8)$$
		and
		$$\alpha_3=(a_9\ a_{10}\ a_{11}\ a_{12}).$$
		In the first second, transmit $\alpha_1$ as the information vector using $v_1$, in the second second, transmit $\alpha_2$, and in the 	third second, transmit $\alpha_3$. After 3 s, all the nodes in $\Im_4$ can recover $\alpha_1$, $\alpha_2$, and $\alpha_3$. Throughout this example, we assume that all transmissions and computations are instantaneous.
		\item[b)] Let $r$ be a vector in $F^4$ such that $\left \langle \{r\}\right \rangle$ intersects trivially with $v_1(T)$ for all $T$ in $\Im_3$, i.e.,$\langle \{r,v_1(T)\} \rangle=F^4$ for all $T$ in $\Im_3$. Such a vector $r$ can be found when $F$ is sufficiently large because there are a finite number of nodes in $\Im_3$. Define $b_1=\alpha_i r$ for $i=1,2,3$. Now remove incoming edges of nodes in $\Im_4$, if necessary, so that $\text{maxflow}(T)$ becomes 3 if $T$ is in $\Im_4$, otherwise, $\text{maxflow}(T)$ remains unchanged. This is possible by virtue of Lemma 3.4. Let $v_2$ be an LCM on the resulting network with $d=3$. Let $\beta=(b_1\ b_2\ b_3)$ and transmit $\beta$ as the information vector using $v_2$ in the fourth second. Then all the nodes in $\Im_3$ can recover $\beta$ and hence $\alpha_1$, $\alpha_2$, and $\alpha_3$.
		\item[c)] Let $s_1$ and $s_2$ be two vectors in $F^3$ such that $\langle \{s_1,s_2\}\rangle$ intersects with $v_2(T)$ trivially for all $T$ in $\Im_1$, i.e., $\langle \{s_1,s_2,v_2(T)\}\rangle = F^3$ for all $T$ in $\Im_1$. Define $\gamma_i=\beta s_i$ for $i=1,2$. Now remove incoming edges of nodes in $\Im_4$ and $\Im_3$, if necessary, so that $\text{maxflow}(T)$ becomes 1 if $T$ is in $\Im_4$ or $\Im_3$, otherwise, $\text{maxflow}(T)$ remains unchanged. Again, this is possible by virtue of Lemma	3.4. Now let $v_3$ be an LCM on the resulting network with $d=1$. In the fifth and the sixth seconds, transmit $\gamma_1$ and $\gamma_2$ as the information vectors using $v_3$. Then all the	nodes in $\Im_1$ can recover $\beta$.
		\item[d)] Let $t_1$ and $t_2$ be two vectors in $F^4$ such that $\langle \{t_1,t_2\}\rangle$ intersects with $\langle \{r,v_1(T)\}\rangle$ trivially for all $T$ in $\Im_1$, i.e., $\langle \{t_1,t_2,r, v_1(T)\}\rangle = F^4$ for all $T$ in $\Im_1$. Define $\delta_1=\alpha_1t_1$ and $\delta_2=\alpha_1t_2$. In the seventh and eighth seconds, transmit $\delta_1$ and $\delta_2$ as the information vectors using $v_3$. Since all the nodes in $\Im_1$ already knows $b_1$, upon receiving $\delta_1$ and $\delta_2$, $\alpha_1$ can then be recovered.
		\item[e)] Define $\delta_3=\alpha_2t_1$ and $\delta_4=\alpha_2t_2$. In the nineth and tenth seconds, transmit $\delta_3$ and $\delta_4$ as the information vectors using $v_3$. Then $\alpha_2$ can be recovered by all the nodes in $\Im_1$.
		\item[f)] Define $\delta_5=\alpha_3t_1$and $\delta_6=\alpha_3t_2$. In the eleventh and twelveth seconds, transmit $\delta_5$ and $\delta_6$ as the information vectors using $v_3$. Then $\alpha_3$ can be recovered by all the nodes in $\Im_1$.
	\end{itemize}
Let us now give a summary of the preceding scheme. In the $i\text{th}$ second for $1=1,2,3,$ via the generic LCM $v_1$, each node in $\Im_4$ receives all four dimensions of $\alpha_i$, each node in $\Im_3$ receives three dimensions of $\alpha_i$, and each node in $\Im_1$ receives one dimension of $\alpha_1$. In the fourth second, via the generic LCM $v_2$, each node in $\Im_3$ receives the vector $\beta$, which provides the three missing dimensions of $\alpha_1$, $\alpha_2$, and $\alpha_3$(one dimension for each) during the first 3 s of multicast by $v_1$. At the same time, each node in $\Im_1$receives one dimension of $\beta$. Now, in order to recover $\beta$, each node in $\Im_1$ needs to receive the two missing dimensions of $\beta$ during the fourth second. This is achieved by the generic LCM $v_3$ in the fifth and sixth seconds. So far, each node in $\Im_1$ has received one dimension of $\alpha_i$ for $i=1,2,3$ via $v_1$ during the first 3 s, and one dimension of $\alpha_i$ for $i=1,2,3$ from $\beta$ via $v_2$ and $v_3$ during the fourth to sixth seconds. Thus, it remains to provide the six missing dimensions of $\alpha_1$, $\alpha_2$, and$\alpha_3$(two dimensions for each) to each node in $\Im_1$, and this is achieved in the seventh to the twelveth seconds via the generic LCM $v_3$.
\end{exam}
\begin{rema}
	The scheme in Example 3.5 can readily be generalized to arbitrary sets of max-flow values. The details are omitted here.
\end{rema}
\par	
In the scheme in Example 3.5, at the end of the 12-s session, each node receives a message of 12 symbols taken from the base field . Thus, the average information rate arriving at each node over the whole session is 1 symbol/s. The result in [1] asserts that this rate, which is the minimum of the individual max-flow bounds of all the nodes in the network, can be achieved. However, it is seen in our scheme that the nodes in $\Im_4$, $\Im_3$, $\Im_1$ can actually receive the whole message in the first 3, 4, and 12 s, respectively. In this sense, each node in our scheme can receive the message at rate equal to its individual max-flow bound. Thus, our result is somewhat stronger than that in [1]. However, our result does not mean that information can be multicast continually from the source to each node at rate equal to its individual max-flow bound.
\par
If it is not necessary for the nodes in $\Im_1$ to receive the message, i.e., the message is multicast to the nodes in $\Im_4$ and $\Im_3$ only, the session can be terminated after 4 s. Then the average information rate arriving at each node in $\Im_4$ and $\Im_3$ over the truncated session is 3 symbols/s, with the nodes in $\Im_4$ and $\Im_3$ receiving the whole message in the first 3 and 4 s, respectively.

\section{THE TRANSMISSION SCHEME ASSOCIATED WITH AN LCM}\label{sec:scheme}

\end{document}
